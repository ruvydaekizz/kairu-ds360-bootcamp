#!/usr/bin/env python3
"""
Prefect Demand Forecasting Pipeline

Bu script her sabah 09:00 Europe/Istanbul saatinde √ßalƒ±≈üacak otomatik talep tahmin pipeline'ƒ±:
1. Veri y√ºkleme
2. Feature engineering
3. Model eƒüitimi/y√ºkleme
4. Tahmin √ºretme
5. Sonu√ßlarƒ± kaydetme

Prefect Schedule:
- Cron: "0 9 * * *" Europe/Istanbul
- Her g√ºn sabah 9'da √ßalƒ±≈üƒ±r
- Production-ready workflow orchestration

Kullanƒ±m:
python prefect_demand_forecast.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import os
import pickle
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional

# Prefect
try:
    from prefect import task, flow
    PREFECT_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è  Prefect bulunamadƒ±. Normal fonksiyonlar kullanƒ±lacak.")
    PREFECT_AVAILABLE = False
    
    # Mock decorators for non-Prefect environments
    def task(func):
        return func
    def flow(func):
        return func

# LightGBM
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    print("‚ùå LightGBM k√ºt√ºphanesi bulunamadƒ±.")
    LIGHTGBM_AVAILABLE = False

# Preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error

warnings.filterwarnings('ignore')

# ================================
# PREFECT TASKS
# ================================

@task
def load_data_task(artifacts_path: str = "/Users/yaseminarslan/Desktop/ds360_ikincihafta/hafta5/artifacts") -> pd.DataFrame:
    """
    Veri y√ºkleme g√∂revi (P1'in basitle≈ütirilmi≈ü hali)
    
    Ger√ßek production'da:
    - Database connection
    - API calls
    - Data validation
    """
    print("üìÅ Veri y√ºkleme ba≈ülƒ±yor...")
    
    try:
        # Mevcut feature engineered veriyi y√ºkle
        train_path = f'{artifacts_path}/datasets/fe_train.parquet'
        valid_path = f'{artifacts_path}/datasets/fe_valid.parquet'
        
        if os.path.exists(train_path) and os.path.exists(valid_path):
            train_df = pd.read_parquet(train_path)
            valid_df = pd.read_parquet(valid_path)
            full_df = pd.concat([train_df, valid_df]).sort_index()
            
            print(f"   ‚úì Veri y√ºklendi: {full_df.shape}")
            print(f"   ‚Ä¢ Tarih aralƒ±ƒüƒ±: {full_df.index.min()} - {full_df.index.max()}")
            print(f"   ‚Ä¢ √úr√ºn sayƒ±sƒ±: {full_df['item_id'].nunique()}")
            
            return full_df
        else:
            raise FileNotFoundError("Feature engineered veriler bulunamadƒ±. √ñnce √∂nceki scriptleri √ßalƒ±≈ütƒ±rƒ±n.")
            
    except Exception as e:
        print(f"   ‚ùå Veri y√ºkleme hatasƒ±: {e}")
        raise

@task
def feature_engineer_task(data_df: pd.DataFrame) -> pd.DataFrame:
    """
    Feature engineering g√∂revi (P2'den gerekli kƒ±sƒ±m)
    
    Not: Bu √∂rnekte veri zaten feature engineered, 
    ama production'da fresh data i√ßin gerekli
    """
    print("‚öôÔ∏è Feature engineering ba≈ülƒ±yor...")
    
    try:
        # Veri zaten FE edilmi≈ü ama son kontroller
        processed_df = data_df.copy()
        
        # Eksik deƒüer kontrol√º
        missing_counts = processed_df.isnull().sum()
        if missing_counts.sum() > 0:
            print(f"   ‚ö†Ô∏è  {missing_counts.sum()} eksik deƒüer bulundu, doldurulacak")
            # Lag ve rolling features i√ßin forward fill
            processed_df = processed_df.fillna(method='ffill').fillna(0)
        
        # Feature validation
        required_features = ['lag_1', 'lag_7', 'lag_28', 'roll_mean_7', 'roll_mean_28', 
                           'dow', 'dom', 'weekofyear', 'month', 'item_id', 'store_id', 'sales']
        
        missing_features = [f for f in required_features if f not in processed_df.columns]
        if missing_features:
            raise ValueError(f"Eksik √∂zellikler: {missing_features}")
        
        print(f"   ‚úì Feature engineering tamamlandƒ±: {processed_df.shape}")
        print(f"   ‚Ä¢ √ñzellik sayƒ±sƒ±: {len(processed_df.columns)}")
        
        return processed_df
        
    except Exception as e:
        print(f"   ‚ùå Feature engineering hatasƒ±: {e}")
        raise

@task
def train_or_load_model_task(data_df: pd.DataFrame, 
                            artifacts_path: str = "./artifacts") -> Dict:
    """
    Model eƒüitimi veya y√ºkleme g√∂revi
    
    Eƒüer model dosyasƒ± varsa y√ºkle, yoksa eƒüit
    """
    print("ü§ñ Model eƒüitimi/y√ºkleme ba≈ülƒ±yor...")
    
    model_path = f'{artifacts_path}/models/lgbm.pkl'
    
    try:
        # Model dosyasƒ± var mƒ± kontrol et
        if os.path.exists(model_path):
            print("   üìÇ Mevcut model y√ºkleniyor...")
            
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            model = model_data['model']
            feature_cols = model_data['feature_cols']
            label_encoders = model_data['label_encoders']
            
            print(f"   ‚úì Model y√ºklendi: LightGBM")
            print(f"   ‚Ä¢ √ñzellik sayƒ±sƒ±: {len(feature_cols)}")
            print(f"   ‚Ä¢ Model iterasyonu: {model.best_iteration}")
            
            return {
                'model': model,
                'feature_cols': feature_cols,
                'label_encoders': label_encoders,
                'is_new_model': False
            }
            
        else:
            print("   üéØ Yeni model eƒüitiliyor...")
            
            # Kategorik encoding
            label_encoders = {}
            data_encoded = data_df.copy()
            
            for col in ['item_id', 'store_id']:
                if col in data_encoded.columns:
                    le = LabelEncoder()
                    data_encoded[f'{col}_encoded'] = le.fit_transform(data_encoded[col])
                    label_encoders[col] = le
            
            # Feature/target ayƒ±rma
            feature_cols = [col for col in data_encoded.columns if col in [
                'lag_1', 'lag_7', 'lag_28', 'roll_mean_7', 'roll_mean_28',
                'dow', 'dom', 'weekofyear', 'month', 'item_id_encoded', 'store_id_encoded'
            ]]
            
            X = data_encoded[feature_cols].fillna(0)
            y = data_encoded['sales']
            
            # Train/validation split (son %10 validation)
            split_idx = int(len(X) * 0.9)
            X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
            y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
            
            # LightGBM parametreleri
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'learning_rate': 0.05,
                'num_leaves': 31,
                'verbose': -1,
                'random_state': 42
            }
            
            # Model eƒüitimi
            train_data = lgb.Dataset(X_train, label=y_train)
            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
            
            model = lgb.train(
                params,
                train_data,
                valid_sets=[val_data],
                num_boost_round=500,
                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
            )
            
            # Model kaydet
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            model_data = {
                'model': model,
                'feature_cols': feature_cols,
                'label_encoders': label_encoders,
                'training_date': datetime.now().isoformat()
            }
            
            with open(model_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            print(f"   ‚úì Model eƒüitimi tamamlandƒ± ve kaydedildi")
            print(f"   ‚Ä¢ Best iteration: {model.best_iteration}")
            
            return {
                'model': model,
                'feature_cols': feature_cols,
                'label_encoders': label_encoders,
                'is_new_model': True
            }
            
    except Exception as e:
        print(f"   ‚ùå Model eƒüitimi/y√ºkleme hatasƒ±: {e}")
        raise

@task
def predict_task(data_df: pd.DataFrame, 
                model_info: Dict, 
                forecast_days: int = 7) -> pd.DataFrame:
    """
    Tahmin √ºretme g√∂revi
    
    Son g√ºne kadar g√ºncelle ve +7 g√ºn (veya +28) tahmin √ºret
    """
    print(f"üîÆ {forecast_days} g√ºnl√ºk tahmin √ºretiliyor...")
    
    try:
        model = model_info['model']
        feature_cols = model_info['feature_cols']
        label_encoders = model_info['label_encoders']
        
        # Son tarihi bul
        last_date = data_df.index.max()
        forecast_start = last_date + timedelta(days=1)
        
        print(f"   ‚Ä¢ Son veri tarihi: {last_date}")
        print(f"   ‚Ä¢ Tahmin ba≈ülangƒ±cƒ±: {forecast_start}")
        
        # Her √ºr√ºn i√ßin tahmin
        all_predictions = []
        unique_items = data_df['item_id'].unique()
        
        for item_id in unique_items:
            # √úr√ºn verisini al
            item_data = data_df[data_df['item_id'] == item_id].copy()
            
            if len(item_data) == 0:
                continue
            
            # Son satƒ±rƒ± template olarak kullan
            last_row = item_data.iloc[-1].copy()
            
            # Kategorik encoding
            for col, le in label_encoders.items():
                if col in last_row:
                    try:
                        last_row[f'{col}_encoded'] = le.transform([last_row[col]])[0]
                    except ValueError:
                        last_row[f'{col}_encoded'] = 0  # Unseen value
            
            # Her g√ºn i√ßin tahmin
            for day in range(forecast_days):
                forecast_date = forecast_start + timedelta(days=day)
                
                # Tarih √∂zelliklerini g√ºncelle
                current_features = last_row.copy()
                current_features['dow'] = forecast_date.weekday()
                current_features['dom'] = forecast_date.day
                current_features['weekofyear'] = forecast_date.isocalendar()[1]
                current_features['month'] = forecast_date.month
                
                # Tahmin yap
                X_pred = current_features[feature_cols].values.reshape(1, -1)
                y_pred = model.predict(X_pred, num_iteration=model.best_iteration)[0]
                y_pred = max(0, y_pred)  # Negatif deƒüerleri 0 yap
                
                # Sonucu kaydet
                all_predictions.append({
                    'date': forecast_date,
                    'item_id': item_id,
                    'store_id': last_row['store_id'],
                    'predicted_sales': y_pred,
                    'model_type': 'LightGBM',
                    'forecast_horizon': day + 1
                })
        
        # DataFrame'e √ßevir
        predictions_df = pd.DataFrame(all_predictions)
        
        print(f"   ‚úì {len(predictions_df)} tahmin √ºretildi")
        print(f"   ‚Ä¢ √úr√ºn sayƒ±sƒ±: {predictions_df['item_id'].nunique()}")
        print(f"   ‚Ä¢ Ortalama tahmin: {predictions_df['predicted_sales'].mean():.2f}")
        
        return predictions_df
        
    except Exception as e:
        print(f"   ‚ùå Tahmin hatasƒ±: {e}")
        raise

@task
def save_outputs_task(predictions_df: pd.DataFrame, 
                     run_date: str,
                     artifacts_path: str = "./artifacts") -> Dict[str, str]:
    """
    Sonu√ßlarƒ± kaydetme g√∂revi
    
    CSV ve PNG dosyalarƒ±nƒ± ./artifacts/preds/ klas√∂r√ºne kaydet
    """
    print("üíæ Sonu√ßlar kaydediliyor...")
    
    try:
        # √áƒ±ktƒ± klas√∂r√º
        output_dir = f'{artifacts_path}/preds'
        os.makedirs(output_dir, exist_ok=True)
        
        # Dosya adlarƒ±
        run_date_str = datetime.strptime(run_date, '%Y-%m-%d').strftime('%Y%m%d')
        csv_path = f'{output_dir}/run_{run_date_str}.csv'
        png_path = f'{output_dir}/run_{run_date_str}_summary.png'
        
        # 1. CSV kaydet
        predictions_df.to_csv(csv_path, index=False)
        print(f"   ‚úì CSV kaydedildi: {csv_path}")
        
        # 2. √ñzet grafik olu≈ütur
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # G√ºnl√ºk toplam tahmin
        daily_total = predictions_df.groupby('date')['predicted_sales'].sum()
        axes[0, 0].plot(daily_total.index, daily_total.values, marker='o', linewidth=2)
        axes[0, 0].set_title('G√ºnl√ºk Toplam Tahmin')
        axes[0, 0].set_ylabel('Toplam Satƒ±≈ü')
        axes[0, 0].grid(True, alpha=0.3)
        
        # √úr√ºn bazƒ±nda toplam
        item_totals = predictions_df.groupby('item_id')['predicted_sales'].sum().sort_values(ascending=False)
        axes[0, 1].bar(range(len(item_totals)), item_totals.values, alpha=0.7)
        axes[0, 1].set_title('√úr√ºn Bazƒ±nda Toplam Tahmin')
        axes[0, 1].set_ylabel('Toplam Satƒ±≈ü')
        axes[0, 1].set_xticks(range(len(item_totals)))
        axes[0, 1].set_xticklabels(item_totals.index, rotation=45)
        
        # Forecast horizon analizi
        horizon_avg = predictions_df.groupby('forecast_horizon')['predicted_sales'].mean()
        axes[1, 0].plot(horizon_avg.index, horizon_avg.values, marker='s', linewidth=2)
        axes[1, 0].set_title('Tahmin Ufku Analizi')
        axes[1, 0].set_xlabel('G√ºn')
        axes[1, 0].set_ylabel('Ortalama Satƒ±≈ü')
        axes[1, 0].grid(True, alpha=0.3)
        
        # √ñzet istatistikler
        axes[1, 1].axis('off')
        stats_text = f"""
Tahmin √ñzeti ({run_date})

‚Ä¢ Toplam tahmin: {len(predictions_df):,}
‚Ä¢ √úr√ºn sayƒ±sƒ±: {predictions_df['item_id'].nunique()}
‚Ä¢ Forecast horizon: {predictions_df['forecast_horizon'].max()} g√ºn
‚Ä¢ Ortalama g√ºnl√ºk satƒ±≈ü: {predictions_df['predicted_sales'].mean():.1f}
‚Ä¢ Toplam beklenen satƒ±≈ü: {predictions_df['predicted_sales'].sum():.0f}
‚Ä¢ Min tahmin: {predictions_df['predicted_sales'].min():.1f}
‚Ä¢ Max tahmin: {predictions_df['predicted_sales'].max():.1f}

Model: LightGBM
Pipeline: Prefect Automated
√áalƒ±≈üma zamanƒ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        """
        axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=11, verticalalignment='top', 
                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.suptitle(f'Demand Forecast Summary - {run_date}', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        # PNG kaydet
        plt.savefig(png_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   ‚úì PNG kaydedildi: {png_path}")
        
        # Sonu√ß √∂zeti
        summary = {
            'csv_path': csv_path,
            'png_path': png_path,
            'total_predictions': len(predictions_df),
            'n_items': int(predictions_df['item_id'].nunique()),
            'forecast_days': int(predictions_df['forecast_horizon'].max()),
            'total_expected_sales': float(predictions_df['predicted_sales'].sum()),
            'avg_daily_sales': float(predictions_df['predicted_sales'].mean())
        }
        
        print(f"   ‚úì √áƒ±ktƒ±lar ba≈üarƒ±yla kaydedildi")
        
        return summary
        
    except Exception as e:
        print(f"   ‚ùå √áƒ±ktƒ± kaydetme hatasƒ±: {e}")
        raise

# ================================
# PREFECT FLOW
# ================================

@flow
def demand_forecast_flow(run_date: Optional[str] = None, 
                        forecast_days: int = 7,
                        artifacts_path: str = "./artifacts") -> Dict:
    """
    Ana demand forecasting flow
    
    Parameters:
    - run_date: √áalƒ±≈üma tarihi (YYYY-MM-DD format, default: bug√ºn)
    - forecast_days: Ka√ß g√ºn tahmin (default: 7)
    - artifacts_path: Artifacts klas√∂r yolu
    
    Schedule: Cron "0 9 * * *" Europe/Istanbul
    - Her g√ºn sabah 09:00'da √ßalƒ±≈üƒ±r
    - ƒ∞stanbul saati ile (T√ºrkiye timezone)
    - Production ortamƒ±nda Prefect server/cloud ile y√∂netilir
    """
    
    # Default run date
    if run_date is None:
        run_date = datetime.now().strftime('%Y-%m-%d')
    
    print("üè™ DEMAND FORECAST PIPELINE BA≈ûLIYOR")
    print("=" * 50)
    print(f"üìÖ √áalƒ±≈üma tarihi: {run_date}")
    print(f"üîÆ Forecast horizon: {forecast_days} g√ºn")
    print(f"üìÅ Artifacts path: {artifacts_path}")
    print(f"‚è∞ Ba≈ülangƒ±√ß zamanƒ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if PREFECT_AVAILABLE:
        print("‚úÖ Prefect modu: Task orchestration aktif")
    else:
        print("‚ö†Ô∏è  Normal mod: Sequential execution")
    
    print("-" * 50)
    
    try:
        # 1. Veri y√ºkleme
        data_df = load_data_task(artifacts_path)
        
        # 2. Feature engineering
        processed_df = feature_engineer_task(data_df)
        
        # 3. Model eƒüitimi/y√ºkleme
        model_info = train_or_load_model_task(processed_df, artifacts_path)
        
        # 4. Tahmin √ºretme
        predictions_df = predict_task(processed_df, model_info, forecast_days)
        
        # 5. Sonu√ßlarƒ± kaydetme
        output_summary = save_outputs_task(predictions_df, run_date, artifacts_path)
        
        # Pipeline √∂zeti
        pipeline_summary = {
            'run_date': run_date,
            'forecast_days': forecast_days,
            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_shape': list(data_df.shape),
            'model_info': {
                'type': 'LightGBM',
                'is_new_model': model_info['is_new_model'],
                'feature_count': len(model_info['feature_cols'])
            },
            'output_summary': output_summary,
            'status': 'SUCCESS'
        }
        
        print("-" * 50)
        print("üéâ PIPELINE BA≈ûARIYLA TAMAMLANDI!")
        print(f"üìä Toplam tahmin: {output_summary['total_predictions']}")
        print(f"üéØ √úr√ºn sayƒ±sƒ±: {output_summary['n_items']}")
        print(f"üí∞ Beklenen toplam satƒ±≈ü: {output_summary['total_expected_sales']:.0f}")
        print(f"üìÅ CSV: {output_summary['csv_path']}")
        print(f"üìà PNG: {output_summary['png_path']}")
        print(f"‚è∞ Biti≈ü zamanƒ±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return pipeline_summary
        
    except Exception as e:
        print("-" * 50)
        print(f"‚ùå PIPELINE HATASI: {e}")
        
        error_summary = {
            'run_date': run_date,
            'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'error': str(e),
            'status': 'FAILED'
        }
        
        raise

# ================================
# SCHEDULING & MAIN
# ================================

def setup_schedule():
    """
    Prefect schedule setup (eƒüitim ama√ßlƒ± g√∂sterim)
    
    Ger√ßek production i√ßin:
    1. prefect deployment build prefect_demand_forecast.py:demand_forecast_flow -n "daily-forecast"
    2. prefect deployment apply demand_forecast_flow-deployment.yaml
    3. prefect agent start -q default
    """
    
    if not PREFECT_AVAILABLE:
        print("‚ö†Ô∏è  Prefect mevcut deƒüil, schedule setup atlanƒ±yor")
        return
    
    try:
        from prefect.deployments import Deployment
        from prefect.server.schemas.schedules import CronSchedule
        
        # Cron schedule: Her g√ºn 09:00 Europe/Istanbul
        schedule = CronSchedule(
            cron="0 9 * * *",  # Dakika Saat G√ºn Ay Haftanƒ±n-g√ºn√º
            timezone="Europe/Istanbul"
        )
        
        deployment = Deployment.build_from_flow(
            flow=demand_forecast_flow,
            name="daily-demand-forecast",
            description="Her sabah 09:00'da √ßalƒ±≈üan otomatik talep tahmin pipeline'ƒ±",
            schedule=schedule,
            parameters={
                "forecast_days": 7,
                "artifacts_path": "./artifacts"
            },
            tags=["production", "forecasting", "daily"]
        )
        
        print("üìÖ Prefect Deployment hazƒ±rlandƒ±:")
        print(f"   ‚Ä¢ Schedule: 0 9 * * * (her g√ºn 09:00)")
        print(f"   ‚Ä¢ Timezone: Europe/Istanbul")
        print(f"   ‚Ä¢ Name: daily-demand-forecast")
        
        # Deployment'ƒ± apply etmek i√ßin:
        # deployment.apply()
        
        return deployment
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Schedule setup hatasƒ±: {e}")
        return None

def main():
    """Ana √ßalƒ±≈ütƒ±rma fonksiyonu"""
    
    print("üöÄ PREFECT DEMAND FORECASTING PIPELINE")
    print("=" * 60)
    
    if not LIGHTGBM_AVAILABLE:
        print("‚ùå LightGBM gerekli")
        return
    
    print("üìã Pipeline Bilgileri:")
    print("   ‚Ä¢ Schedule: Her g√ºn 09:00 Europe/Istanbul")
    print("   ‚Ä¢ Tasks: Load ‚Üí FE ‚Üí Model ‚Üí Predict ‚Üí Save")
    print("   ‚Ä¢ Output: CSV + PNG reports")
    print("   ‚Ä¢ Orchestration: Prefect Tasks & Flows")
    
    if PREFECT_AVAILABLE:
        print("   ‚Ä¢ Prefect: ‚úÖ Aktif")
    else:
        print("   ‚Ä¢ Prefect: ‚ö†Ô∏è  Mock mode")
    
    print("\nüí° Production Deployment:")
    print("   1. prefect deployment build prefect_demand_forecast.py:demand_forecast_flow -n daily-forecast")
    print("   2. prefect deployment apply demand_forecast_flow-deployment.yaml") 
    print("   3. prefect agent start -q default")
    print("   4. Prefect UI: http://localhost:4200")
    
    print("\n" + "=" * 60)
    
    try:
        # Schedule setup (eƒüitim ama√ßlƒ±)
        deployment = setup_schedule()
        
        # Test √ßalƒ±≈ümasƒ±
        print("\nüß™ Test √ßalƒ±≈ümasƒ± ba≈ülatƒ±lƒ±yor...")
        result = demand_forecast_flow(
            run_date=datetime.now().strftime('%Y-%m-%d'),
            forecast_days=7
        )
        
        print(f"\n‚úÖ Test ba≈üarƒ±lƒ±!")
        print(f"üìä Status: {result['status']}")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Kullanƒ±cƒ± tarafƒ±ndan durduruldu")
    except Exception as e:
        print(f"\n‚ùå Hata: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()